---
title: "Multiple Linear Regression"
author: "Ian Curtis"
date: "2024-03-12"
output: html_document
---

# Welcome

Hey there! This is the second guide in my Modeling Guide with R series. In the previous guide, we performed some preliminary explorations on our dataset and took a look at a few of the variables. We cleaned the dataset up a bit and exported it so we could read it again in later guides.

This page will focus on both simple and multiple linear regression. Although, I cannot cover all of the details and fine use cases of linear regression, we will explore some of the key ideas to focus on when creating and analyzing a linear regression model. We'll start by importing our data and identifying variables to use in the model then move to creating, interpreting, and testing the model. In both simple and linear regression, we'll use train/test validation and we'll also look at several candidate models.

We will be using the same dataset for both the simple and multiple regression cases so let's get that set up first. As mentioned in the previous, guide, I prefer to use the `tidyverse` family of packages. We'll also be using the `tidymodels` collection of packages to set up the models and perform our train/test validation and `GGally` to make a scatterplot matrix.

The code below imports our packages and the data we cleaned in the previous guide. It also splits the data into a train and test set. We will train our models using the training set and will test its performance using the test set. We do this to simulate how the model will perform in the "real world".

I will be setting the seed for the random generator so we can get the same results each time I start a new R session and split the data. You may or may not wish to do this in your work. I want to make sure we get a roughly equal amount of each store in the split sets so I will take a stratified sample to make the split (`strata = store`).

```{r load-items}
library(tidyverse)
library(tidymodels)
library(GGally)

retail <- read_csv(here::here("data/retail_clean.csv"))

set.seed(52319)
retail_split <- initial_split(retail, prop = .8, strata = store)

train <- training(retail_split)
test <- testing(retail_split)
```

Alright! Now that we have that set, we'll use the `train` data for all the model creation and will only use the `test` data when we are analyzing our models.

# Multiple Linear Regression

Above, we weren't able to predict `rating` with one variable alone. What if we could use multiple variables in regression model? Maybe then we can explain more variance in `rating`. 

## Requirements

When can we use multiple linear regression (MLR)?

* The response variable (the variable we are predicting) must be continuous.
* The predictor variable (the variable we are using to predict) can be either continuous or categorical with any amount of levels.
* We only get one response variable but can have as many predictor variables as we wish.

We also still have to meet the regression assumptions:

* The relationship between the predictor variable and the response variables should be linear
* The errors of our predictions should be approximately normally distributed
* Our predictor variables should not be correlated with each other
* There should be a constant variability in our errors
* Our errors should be independent (we will not focus on this here)

## Selecting Variables

The tricky part about working with MLR is that you want to find the right balance of variables. We *do* want to choose variables we think might have an effect on `rating` but we *don't* want to include any unnecessary variables or variables that don't provide much benefit to the model.

As a result, MLR is an iterative process. I like to start out by creating a full model that has every single predictor variable that I think could be useful in predicting my response variable. I then use the output from the model to try and pull out the variables that are not pulling their weight.

So let's start with that full model. These are the variables that I think might be useful (remember that in MLR we can use categorical variables):

* Store
* Customer Type
* Gender
* Total
* Unit Price
* Pay Method

We will start by creating a model that includes all of these variables and go from there.

## Building the Model

Because we are still working in linear regression, we can use the same `lm_spec` engine from above to drive our model building. This time, we'll use a different formula to fit the model. Notice we are back on the training data because we are training a new model. We once again will take the log of `total`.

```{r mlr-full-fit}
mlr_mod_full <- lm_spec %>% 
  fit(rating ~ store + customer_type + gender + log(total) + unit_price + pay_method, data = train)
```

Once we have done that, we can look at the output:

```{r mlr-full-output}
tidy(mlr_mod_full)
```

We see that R has created dummy variables for us for each of our categorical variables. We also see, once again, that the values in the `estimate` column are very small. It seems that each variable does not do a lot on it's own to predict `rating`. That being said, each individual variable is statistically significant indicating that each variable may provide some insight into predicting rating when used with the other variables.

Let's see what the $R^2$ value is to see if we explained any more variance in `rating` than with SLR.

```{r}
glance(mlr_mod_full)
```

Looks like we explain 0.292% of the variance in rating! This is a small increase from before, even by including quite a few more variables. $R^2$ will always increase when including more variables and the fact that we still haven't explained even 1% of the variance in `rating` means that we still aren't doing a great job. 

Even though this model isn't great, I do want to explore a different model by removing some of the variables to see if we can do equally as good with fewer variables. Why have extra variables in the model if we can do equally as good or slightly worse with a model that has less variables?

Looking at the regression output, the log of `total` and `unit_price` seem to have the least effect in terms of their `estimate`. Let's remove them and see if anything changes.

```{r mlr-reduced}
mlr_mod_reduced <- lm_spec %>% 
  fit(rating ~ store + customer_type + gender + pay_method, data = train)
tidy(mlr_mod_reduced)
glance(mlr_mod_reduced)
```

We see that we still explain 0.282% of the variation in `rating` even by removing two variables. I think it is justified to keep these two out of the (admittedly poor) model.

## Assessing the Model

Now let's assess the model and see if regression was a good choice. We already have seen indications that it is not but it's always good to check the assumptions.

### Linearity

First, can we assume linearity between the response and the predictor?

As with SLR, we can use the errors for this. First, we need to calculate the errors for the training dataset (we previously found the errors for the testing dataset). Then we can plot the errors versus the predicted values.

```{r}
train_aug <- augment(mlr_mod_reduced, new_data = train)

ggplot(data = train_aug, aes(x = .pred, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Errors vs. Predicted",
       subtitle = "MLR Reduced Model") +
  xlab("Fitted values") +
  ylab("Errors")
```

We are looking for a random scatter of points (and not a pattern). We do see a random scatter here with no general pattern but I do see a column-like pattern. Also, the model does not predict ratings below 6.8 or higher than 7.3 which means we have some pretty high errors terms, seen with the points above 2 and below -2.

### Constant Variability of Errors

Using the above plot, do we see any curvature around the dashed line? Are the errors more spread out (vertically) at some points than others? As before, I do not see evidence to say that we have violated this assumption. At nearly every fitted value, the residuals span from about -2.5 to 2.5.

### Normal Errors

We can make a histogram of the errors to check for the normality assumption.

```{r}
ggplot(data = train_aug, aes(x = .resid)) +
  geom_histogram(binwidth = 0.25, fill = "#099392", color = "black") +
  xlab("Residuals")
```

I would say we meet this assumption as well!

### Colinearity Between Predictors

We also should make sure that our predictor variables are not correlated with each other. If they are, we can consider create an interaction term (which would indicate that the two variables work together) or we should remove one of the correlated variables (under the theory that if they are both related and/or measuring the same thing, then only one is needed). We can look at all of this by creating a plot matrix.

```{r corr-mtx}
train_subset <- train %>% 
  select(-c(id, product_class, qty, total, unit_price, date, time, subtotal))

ggpairs(train_subset)
```

This is not intended to be a detailed look at all pairs of variables but can give us some insight into the distributions of each individual variable as well as what things look like across variables. I don't see any need for an interaction variable nor do I think that we have evidence for any correlation across predictors.

Let's also take a quick peek at our testing data. How well does the model perform?

```{r mlr-test}
test_aug <- augment(mlr_mod_reduced, new_data = test)

ggplot(data = test_aug, aes(x = .pred, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Errors vs. Predicted",
       subtitle = "MLR Reduced Model Testing Dataset") +
  xlab("Fitted values") +
  ylab("Errors")
```

It looks like we still see a big range in our errors (between -3 and 3). This is a big range for errors. We also see again that the fitted values range from 6.8 to about 7.3 which doesn't not represent the `rating` variable accurately.

## Wrap Up

All in all, regression was not the best model to use to predict `rating`. The variables in the dataset simply do not hold enough information in order to make good predictions. We saw very low $R^2$ values, very small estimates for our linear model, and a high error rate. We also saw some slightly concerning error plots indicating that the regression assumptions may not be met which further questions the validity and accuracy of our results.



